import joblib
import shutil
from src.reporting import generate_pdf_report
from src.tuning import (
    tune_logistic_regression,
    tune_random_forest,
    tune_gradient_boosting,
    tune_XGboost,
    tune_LightGBM
)
from src.modeling import (
    load_processed_data,
    save_model,
    build_metrics_dataframe,
    plot_model_metrics,
    select_best_model
)


def main():
    # Carregar dados processados
    print("üìÇ Carregando dados processados...")
    X_train, y_train, X_test, y_test = load_processed_data()

    # Tuning dos modelos (opcional)
    print("\n‚öôÔ∏è Iniciando tuning de hiperpar√¢metros...")

    # 1) Logistic Regression
    lr_tuned = tune_logistic_regression(X_train, y_train, X_test, y_test)
    print("‚úî Logistic Regression tunado.")

    # 2) Random Forest
    rf_tuned = tune_random_forest(X_train, y_train, X_test, y_test)
    print("‚úî Random Forest tunado.")

    # 3) Gradient Boosting
    gb_tuned = tune_gradient_boosting(X_train, y_train, X_test, y_test)
    print("‚úî Gradient Boosting tunado.")

    # 4) XGBoost
    xgb_tuned = tune_XGboost(X_train, y_train, X_test, y_test)
    print("‚úî XGBoost tunado.")

    # 5) LightGBM
    lgbm_tuned = tune_LightGBM(X_train, y_train, X_test, y_test)
    print("‚úî LightGBM tunado.")
    print("‚úî Tuning de hiperpar√¢metros conclu√≠do.")

    # Avalia√ß√£o dos modelos
    print("\nüìä Avaliando modelos...")

    
    lr_tuned_metrics = lr_tuned['final_metrics']
    rf_tuned_metrics = rf_tuned['final_metrics']
    gb_tuned_metrics = gb_tuned['final_metrics']
    xgb_tuned_metrics = xgb_tuned['final_metrics']
    lgbm_tuned_metrics = lgbm_tuned['final_metrics']


    results_tuned = {
    "Logistic Regression (Tuned)": lr_tuned_metrics,
    "Random Forest (Tuned)": rf_tuned_metrics,
    "Gradient Boosting (Tuned)": gb_tuned_metrics,
    "XGBoost (Tuned)": xgb_tuned_metrics,
    "LightGBM (Tuned)": lgbm_tuned_metrics
    }

    # Salvar modelos
    print("\nüíæ Salvando modelos...")
    save_model(lr_tuned['best_model'], "models/logistic_regression_tuned.pkl")
    save_model(rf_tuned['best_model'], "models/random_forest_tuned.pkl")
    save_model(gb_tuned['best_model'], "models/gradient_boosting_tuned.pkl")
    save_model(xgb_tuned['best_model'], "models/xgboost_tuned.pkl")
    save_model(lgbm_tuned['best_model'], "models/lightgbm_tuned.pkl")
    

    print("\n‚úî Todos os modelos foram treinados e salvos com sucesso em /models/")


    def print_model_comparison(results):
        print("\nüìä Compara√ß√£o de Modelos:")
        print("Modelo           ROC-AUC    Recall    Precision")
        print("-----------------------------------------------")
        for model_name, metrics in results.items():
            print(f"{model_name:20} {metrics['roc_auc']:.4f}   {metrics['recall']:.4f}   {metrics['precision']:.4f}")

    print_model_comparison(results_tuned)

    # Construir DataFrame de m√©tricas
    df_tuned = build_metrics_dataframe(results_tuned)
    plot_model_metrics(df_tuned)

    # Gerar relat√≥rios PDF
    generate_pdf_report(df_tuned)

    # Mostrar o melhor modelo
    best_model_info = select_best_model(df_tuned)
    print("\nüèÜ Melhor modelo selecionado:")
    print(best_model_info)

    # Copiar modelo vencedor para modelo_final.pkl
    shutil.copy(
        best_model_info["best_model_path"],
        "models/modelo_final.pkl"
    )

    print("‚úÖ Modelo final salvo em models/modelo_final.pkl")

        

if __name__ == "__main__":
    main()
